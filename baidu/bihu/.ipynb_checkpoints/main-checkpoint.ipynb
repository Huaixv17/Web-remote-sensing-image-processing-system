{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5a82093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集划分已完成。\n"
     ]
    }
   ],
   "source": [
    "# 划分训练集/验证集，并生成文件名列表\n",
    "\n",
    "import random\n",
    "import os.path as osp\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "# 随机数生成器种子\n",
    "RNG_SEED = 114514\n",
    "# 调节此参数控制训练集数据的占比\n",
    "TRAIN_RATIO = 0.80\n",
    "# 数据集路径\n",
    "DATA_DIR = 'C:/Users/Administrator/diwu_pre/baidu/bihu/data/'\n",
    "\n",
    "# 处理测试集\n",
    "\n",
    "\n",
    "print(\"数据集划分已完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7555ac6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\paddle\\lib\\site-packages\\sklearn\\utils\\multiclass.py:13: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.\n",
      "  from scipy.sparse.base import spmatrix\n",
      "D:\\Anaconda3\\envs\\paddle\\Lib\\site-packages\\paddlers-0.0.1-py3.9.egg\\paddlers\\models\\ppcls\\data\\preprocess\\ops\\timm_autoaugment.py:38: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  _RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\n",
      "D:\\Anaconda3\\envs\\paddle\\Lib\\site-packages\\paddlers-0.0.1-py3.9.egg\\paddlers\\models\\ppcls\\data\\preprocess\\ops\\timm_autoaugment.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  _RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\n",
      "2022-07-05 10:34:06,834-WARNING: type object 'QuantizationTransformPass' has no attribute '_supported_quantizable_op_type'\n",
      "2022-07-05 10:34:06,835-WARNING: If you want to use training-aware and post-training quantization, please use Paddle >= 1.8.4 or develop version\n"
     ]
    }
   ],
   "source": [
    "# 导入一些需要用到的库\n",
    "\n",
    "import random\n",
    "import os\n",
    "import os.path as osp\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "import math\n",
    "import paddlers.models.ppseg as ppseg\n",
    "from paddle.optimizer import lr\n",
    "import numpy as np\n",
    "import skimage\n",
    "import cv2\n",
    "import paddle\n",
    "import paddlers as pdrs\n",
    "from paddlers import transforms as T\n",
    "from skimage.io import imread, imsave\n",
    "from PIL import Image,ImageEnhance\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93003311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义全局变量\n",
    "# 可在此处调整实验所用超参数\n",
    "\n",
    "# 随机种子\n",
    "SEED = 1919810\n",
    "\n",
    "# 数据集路径\n",
    "DATA_DIR = 'C:/Users/Administrator/diwu_pre/baidu/bihu/data/'\n",
    "# 实验路径。实验目录下保存输出的模型权重和结果\n",
    "EXP_DIR = 'C:/Users/Administrator/diwu_pre/baidu/bihu/exp/'\n",
    "#输出保存路径\n",
    "EXP_DIR_R = 'C:/Users/Administrator/diwu_pre/baidu/bihu/'\n",
    "# 保存最佳模型的路径\n",
    "BEST_CKP_PATH = osp.join(EXP_DIR, 'model.pdparams')\n",
    "\n",
    "# 训练的epoch数\n",
    "NUM_EPOCHS = 150\n",
    "# 每多少个epoch保存一次模型权重参数\n",
    "SAVE_INTERVAL_EPOCHS = 5\n",
    "# 初始学习率\n",
    "LR = 4e-4\n",
    "# 学习率衰减步长（注意，单位为迭代次数而非epoch数），即每多少次迭代将学习率衰减一半\n",
    "# DECAY_STEP = 1000\n",
    "# 训练阶段 batch size\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "# 推理阶段 batch size\n",
    "INFER_BATCH_SIZE = 32\n",
    "# 加载数据所使用的进程数\n",
    "NUM_WORKERS = 4\n",
    "# 裁块大小\n",
    "CROP_SIZE = 336\n",
    "# 模型推理阶段使用的滑窗步长\n",
    "STRIDE = 64\n",
    "# 影像原始大小\n",
    "ORIGINAL_SIZE = (1024, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42b141e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<paddle.fluid.core_avx.Generator at 0x1938ab8f430>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 固定随机种子，尽可能使实验结果可复现\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "paddle.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14f3503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一些辅助函数\n",
    "\n",
    "def info(msg, **kwargs):\n",
    "    print(msg, **kwargs)\n",
    "\n",
    "\n",
    "def warn(msg, **kwargs):\n",
    "    print('\\033[0;31m'+msg, **kwargs)\n",
    "\n",
    "\n",
    "def quantize(arr):\n",
    "    return (arr*255).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a23e721e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2022-07-05 10:34:23,642 download.py:119] unique_endpoints {''}\n",
      "INFO 2022-07-05 10:34:23,644 download.py:254] Downloading resnet34.pdparams from https://paddle-hapi.bj.bcebos.com/models/resnet34.pdparams\n",
      "100%|████████████████████████████████████████████████████████████████████████| 128669/128669 [00:18<00:00, 6906.96it/s]\n",
      "INFO 2022-07-05 10:34:42,774 download.py:273] File C:\\Users\\Administrator/.cache/paddle/hapi/weights\\resnet34.pdparams md5 checking...\n"
     ]
    }
   ],
   "source": [
    "# 调用PaddleRS API一键构建模型\n",
    "model = pdrs.tasks.BIT(\n",
    "    # 模型输出类别数\n",
    "    num_classes=2,\n",
    "    # 是否使用混合损失函数，默认使用交叉熵损失函数训练\n",
    "    use_mixed_loss=False,\n",
    "    # 模型输入通道数\n",
    "    in_channels=3,\n",
    "    # 模型使用的骨干网络，支持'resnet18'或'resnet34'\n",
    "    backbone='resnet34',\n",
    "    # 骨干网络中的resnet stage数量\n",
    "    n_stages=4,\n",
    "    # 是否使用tokenizer获取语义token\n",
    "    use_tokenizer=True,\n",
    "    # token的长度\n",
    "    token_len=6,\n",
    "    # 若不使用tokenizer，则使用池化方式获取token。此参数设置池化模式，有'max'和'avg'两种选项，分别对应最大池化与平均池化\n",
    "    pool_mode='max',\n",
    "    # 池化操作输出特征图的宽和高（池化方式得到的token的长度为pool_size的平方）\n",
    "    pool_size=2,\n",
    "    # 是否在Transformer编码器中加入位置编码（positional embedding）\n",
    "    enc_with_pos=True,\n",
    "    # Transformer编码器使用的注意力模块（attention block）个数\n",
    "    enc_depth=1,\n",
    "    # Transformer编码器中每个注意力头的嵌入维度（embedding dimension）\n",
    "    enc_head_dim=64,\n",
    "    # Transformer解码器使用的注意力模块个数\n",
    "    dec_depth=8,\n",
    "    # Transformer解码器中每个注意力头的嵌入维度\n",
    "    dec_head_dim=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72caf26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建需要使用的数据变换（数据增强、预处理）\n",
    "# 使用Compose组合多种变换方式。Compose中包含的变换将按顺序串行执行\n",
    "\n",
    "eval_transforms = T.Compose([\n",
    "    # 在验证阶段，输入原始尺寸影像，对输入影像仅进行归一化处理\n",
    "    # 验证阶段与训练阶段的数据归一化方式必须相同\n",
    "    T.Normalize(\n",
    "        mean=[0.5, 0.5, 0.5],\n",
    "        std=[0.5, 0.5, 0.5]\n",
    "    )\n",
    "])\n",
    "\n",
    "# 实例化数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed0ace5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 若实验目录不存在，则新建之（递归创建目录）\n",
    "if not osp.exists(EXP_DIR):\n",
    "    os.makedirs(EXP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "407340fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义推理阶段使用的数据集\n",
    "\n",
    "class InferDataset(paddle.io.Dataset):\n",
    "    \"\"\"\n",
    "    变化检测推理数据集。\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): 数据集所在的目录路径。\n",
    "        transforms (paddlers.transforms.Compose): 需要执行的数据变换操作。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        transforms\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.transforms = deepcopy(transforms)\n",
    "\n",
    "        pdrs.transforms.arrange_transforms(\n",
    "            model_type='changedetector',\n",
    "            transforms=self.transforms,\n",
    "            mode='test'\n",
    "        )\n",
    "\n",
    "        with open(osp.join(data_dir, 'test.txt'), 'r') as f:\n",
    "            lines = f.read()\n",
    "            lines = lines.strip().split('\\n')\n",
    "\n",
    "        samples = []\n",
    "        names = []\n",
    "        for line in lines:\n",
    "            items = line.strip().split(' ')\n",
    "            items = list(map(pdrs.utils.path_normalization, items))\n",
    "            item_dict = {\n",
    "                'image_t1': osp.join(data_dir, items[0]),\n",
    "                'image_t2': osp.join(data_dir, items[1])\n",
    "            }\n",
    "            samples.append(item_dict)\n",
    "            names.append(osp.basename(items[0]))\n",
    "\n",
    "        self.samples = samples\n",
    "        self.names = names\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.names[idx]\n",
    "        sample = deepcopy(self.samples[idx])\n",
    "        output = self.transforms(sample)\n",
    "        return name, \\\n",
    "               paddle.to_tensor(output[0]), \\\n",
    "               paddle.to_tensor(output[1]),\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dae25d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 考虑到原始影像尺寸较大，以下类和函数与影像裁块-拼接有关。\n",
    "\n",
    "class WindowGenerator:\n",
    "    def __init__(self, h, w, ch, cw, si=1, sj=1):\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        self.ch = ch\n",
    "        self.cw = cw\n",
    "        if self.h < self.ch or self.w < self.cw:\n",
    "            raise NotImplementedError\n",
    "        self.si = si\n",
    "        self.sj = sj\n",
    "        self._i, self._j = 0, 0\n",
    "\n",
    "    def __next__(self):\n",
    "        # 列优先移动（C-order）\n",
    "        if self._i > self.h:\n",
    "            raise StopIteration\n",
    "        \n",
    "        bottom = min(self._i+self.ch, self.h)\n",
    "        right = min(self._j+self.cw, self.w)\n",
    "        top = max(0, bottom-self.ch)\n",
    "        left = max(0, right-self.cw)\n",
    "\n",
    "        if self._j >= self.w-self.cw:\n",
    "            if self._i >= self.h-self.ch:\n",
    "                # 设置一个非法值，使得迭代可以early stop\n",
    "                self._i = self.h+1\n",
    "            self._goto_next_row()\n",
    "        else:\n",
    "            self._j += self.sj\n",
    "            if self._j > self.w:\n",
    "                self._goto_next_row()\n",
    "\n",
    "        return slice(top, bottom, 1), slice(left, right, 1)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def _goto_next_row(self):\n",
    "        self._i += self.si\n",
    "        self._j = 0\n",
    "\n",
    "    \n",
    "def crop_patches(dataloader, ori_size, window_size, stride):\n",
    "    \"\"\"\n",
    "    将`dataloader`中的数据裁块。\n",
    "\n",
    "    Args:\n",
    "        dataloader (paddle.io.DataLoader): 可迭代对象，能够产生原始样本（每个样本中包含任意数量影像）。\n",
    "        ori_size (tuple): 原始影像的长和宽，表示为二元组形式(h,w)。\n",
    "        window_size (int): 裁块大小。\n",
    "        stride (int): 裁块使用的滑窗每次在水平或垂直方向上移动的像素数。\n",
    "\n",
    "    Returns:\n",
    "        一个生成器，能够产生iter(`dataloader`)中每一项的裁块结果。一幅图像产生的块在batch维度拼接。例如，当`ori_size`为1024，而\n",
    "            `window_size`和`stride`均为512时，`crop_patches`返回的每一项的batch_size都将是iter(`dataloader`)中对应项的4倍。\n",
    "    \"\"\"\n",
    "\n",
    "    for name, *ims in dataloader:\n",
    "        ims = list(ims)\n",
    "        h, w = ori_size\n",
    "        win_gen = WindowGenerator(h, w, window_size, window_size, stride, stride)\n",
    "        all_patches = []\n",
    "        for rows, cols in win_gen:\n",
    "            # NOTE: 此处不能使用生成器，否则因为lazy evaluation的缘故会导致结果不是预期的\n",
    "            patches = [im[...,rows,cols] for im in ims]\n",
    "            all_patches.append(patches)\n",
    "        yield name[0], tuple(map(partial(paddle.concat, axis=0), zip(*all_patches)))\n",
    "\n",
    "\n",
    "def recons_prob_map(patches, ori_size, window_size, stride):\n",
    "    \"\"\"从裁块结果重建原始尺寸影像，与`crop_patches`相对应\"\"\"\n",
    "    # NOTE: 目前只能处理batch size为1的情况\n",
    "    h, w = ori_size\n",
    "    win_gen = WindowGenerator(h, w, window_size, window_size, stride, stride)\n",
    "    prob_map = np.zeros((h,w), dtype=np.float)\n",
    "    cnt = np.zeros((h,w), dtype=np.float)\n",
    "    # XXX: 需要保证win_gen与patches具有相同长度。此处未做检查\n",
    "    for (rows, cols), patch in zip(win_gen, patches):\n",
    "        prob_map[rows, cols] += patch\n",
    "        cnt[rows, cols] += 1\n",
    "    prob_map /= cnt\n",
    "    return prob_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1183f5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 若输出目录不存在，则新建之（递归创建目录）\n",
    "out_dir = osp.join(EXP_DIR_R, 'results')\n",
    "if not osp.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "# 为模型加载历史最佳权重\n",
    "state_dict = paddle.load(BEST_CKP_PATH)\n",
    "# 同样通过net属性访问组网对象\n",
    "model.net.set_state_dict(state_dict)\n",
    "\n",
    "# 实例化测试集\n",
    "test_dataset = InferDataset(\n",
    "    DATA_DIR,\n",
    "    # 注意，测试阶段使用的归一化方式需与训练时相同\n",
    "    T.Compose([\n",
    "        T.Normalize(\n",
    "            mean=[0.5, 0.5, 0.5],\n",
    "            std=[0.5, 0.5, 0.5]\n",
    "        )\n",
    "    ])\n",
    ")\n",
    "\n",
    "# 创建DataLoader\n",
    "test_dataloader = paddle.io.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    "    return_list=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1677155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型推理开始\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型推理完成\n"
     ]
    }
   ],
   "source": [
    "# 推理过程主循环\n",
    "info(\"模型推理开始\")\n",
    "\n",
    "model.net.eval()\n",
    "len_test = len(test_dataset)\n",
    "test_patches = crop_patches(\n",
    "    test_dataloader,\n",
    "    ORIGINAL_SIZE,\n",
    "    CROP_SIZE,\n",
    "    STRIDE\n",
    ")\n",
    "with paddle.no_grad():\n",
    "    for name, (t1, t2) in tqdm(test_patches, total=len_test):\n",
    "        shape = paddle.shape(t1)\n",
    "        pred = paddle.zeros(shape=(shape[0],2,*shape[2:]))\n",
    "        for i in range(0, shape[0], INFER_BATCH_SIZE):\n",
    "            pred[i:i+INFER_BATCH_SIZE] = model.net(t1[i:i+INFER_BATCH_SIZE], t2[i:i+INFER_BATCH_SIZE])[0]\n",
    "        # 取softmax结果的第1（从0开始计数）个通道的输出作为变化概率\n",
    "        prob = paddle.nn.functional.softmax(pred, axis=1)[:,1]\n",
    "        # 由patch重建完整概率图\n",
    "        prob = recons_prob_map(prob.numpy(), ORIGINAL_SIZE, CROP_SIZE, STRIDE)\n",
    "        # 默认将阈值设置为0.5，即，将变化概率大于0.5的像素点分为变化类\n",
    "        out = quantize(prob>0.5)\n",
    "\n",
    "        imsave(osp.join(out_dir, 'your_file.png'), out, check_contrast=False)\n",
    "\n",
    "info(\"模型推理完成\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77e4a42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paddle",
   "language": "python",
   "name": "paddle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
